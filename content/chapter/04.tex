%!TEX root = ../../main.tex

\chapter{Ergebnisse}

In diesem Kapitel werden die Ergebnisse der Bewerbervorauswahl vorgestellt. Ziel der Evaluation war es, herauszufinden, welches System besser geeignet ist, um geeignete Kandidaten bereits in der Vorauswahl zu identifizieren. Durch die Analyse der Interaktionen und Bewertungen von Testkandidaten durch beide Systeme sollen fundierte Rückschlüsse auf deren jeweilige Stärken und Schwächen gezogen werden.

Die vom \acs{LLM} gestellten Fragen sowie die Bewertungen der Antworten wurden analysiert, um Rückschlüsse auf die Effektivität des \acs{LLM} in der Vorauswahl von Bewerbern zu ziehen.

\section{Zusammenfassung der Ergebnisse}

Das Experiment, bei dem das \acs{LLM} Bewerber basierend auf den vier vorgegebenen Kategorien bewertete (fachliche Kompetenz, Problemlösungsfähigkeit, Konfliktlösung und Teamfähigkeit), ergab wertvolle Einblicke in die Funktionsweise des \acs{LLM}s. Dennoch wurden einige Schwächen bei der Bewertung festgestellt, die zu einer insgesamt zu positiven Einschätzung der Kandidaten führten.

\textbf{Fachliche Kompetenz:}

\begin{itemize}
    \item Die Bewertung der fachlichen Kompetenz stellte sich als herausfordernd dar. Kandidaten erreichten selten die volle Punktzahl, obwohl ihre technischen Antworten gut waren. Die Schwierigkeit, in dieser Kategorie eine hohe Bewertung zu erzielen, weist darauf hin, dass die Bewertungskriterien für technisches Wissen möglicherweise zu streng waren.
    \item Zudem wurde festgestellt, dass die fachliche Kompetenz stärker gewichtet werden sollte, da sie im IT-Kontext besonders entscheidend ist. Eine Gewichtung, die 2-3-mal so hoch ist wie die der anderen Kategorien, könnte zu differenzierteren Ergebnissen führen.
\end{itemize}

\textbf{Problemlösungsfähigkeit:}

\begin{itemize}
    \item In dieser Kategorie fielen die Bewertungen tendenziell zu hoch aus. Einige Kandidaten konnten durch oberflächliche Antworten eine gute Bewertung erzielen, was darauf hindeutet, dass das \acs{LLM} nicht immer in der Lage war, zwischen tiefen, durchdachten Antworten und oberflächlichem Wissen zu unterscheiden.
    \item Besonders problematisch war es, wenn Kandidaten Technologien oder Konzepte wiederholten, ohne sie tiefer zu erklären. Hier hätte das \acs{LLM} Punkte abziehen sollen.
\end{itemize}

\textbf{Konfliktlösungsfähigkeit:}

\begin{itemize}
    \item Die Kandidaten erhielten auch hier tendenziell hohe Bewertungen, insbesondere wenn sie generische oder standardisierte Antworten gaben. Dies führte dazu, dass Kandidaten mit wenig Erfahrung in der Konfliktlösung zu gut bewertet wurden. Eine strengere Beurteilung der tatsächlichen Konfliktlösungsstrategien wäre notwendig.
\end{itemize}

\textbf{Teamfähigkeit:}

\begin{itemize}
    \item Die Bewertung der Teamfähigkeit war oft positiv und gleichförmig, unabhängig von den individuellen Antworten. Dies deutet darauf hin, dass das \acs{LLM} Schwierigkeiten hatte, differenzierte Rückschlüsse auf die Teamdynamik und individuelle Beiträge zur Teamarbeit zu ziehen.
\end{itemize}

\textbf{Identifizierte Probleme:}

\begin{itemize}
    \item \textbf{Wiederholung von Technologien:} Ein wesentlicher Fehler war, dass Kandidaten, die Technologien oder Begriffe wiederholten, die vom \acs{LLM} vorgegeben wurden, zu hoch bewertet wurden. Diese Methode, die sich als eine Art ``Cheatcode''  herausstellte, führte zu übertrieben positiven Ergebnissen.
    \item \textbf{Mangel an Erklärungen:} Wenn Kandidaten Technologien nicht erklären konnten, wurden keine Punkte abgezogen, was die Bewertung verzerrte. Hier sollte das \acs{LLM} strenger werden und Punkte abziehen, wenn Erklärungen fehlen.
\end{itemize}

\section{Erkenntnisse und Schlussfolgerungen}

Die Analyse der Ergebnisse hat gezeigt, dass der \acs{KI}-Bot in vielen Fällen dazu neigt, Bewerber zu positiv zu bewerten, insbesondere in den Kategorien der Soft Skills wie Teamfähigkeit und Konfliktlösung. Diese Tendenz führte dazu, dass Kandidaten, die teilweise nur oberflächliche Antworten gaben, dennoch hohe Bewertungen erzielten. Trotz dieser Verzerrungen konnte die \acs{KI} klare Unterschiede zwischen den einzelnen Bewerbern erkennen, was eine grundlegende Einteilung in \enquote{geeignet} und \enquote{nicht geeignet} ermöglichte.

Eine zentrale Erkenntnis der Evaluation ist, dass das \acs{LLM} eine vielversprechende Grundlage für die Bewerbervorauswahl bietet, jedoch Anpassungen notwendig sind, um die Differenzierung zwischen den Kandidaten zu verbessern. Die derzeitigen Bewertungsmechanismen sind noch nicht ausgereift genug, um komplexe Fähigkeiten wie Problemlösung oder die Anwendung von Fachwissen präzise zu bewerten.

\subsection{Herausforderungen und Grenzen}

Im Verlauf der Evaluation wurden mehrere Herausforderungen und Grenzen des \acs{LLM}-gestützten Bewertungssystems deutlich:

\begin{itemize}
    \item \textbf{Optimierung des Fragebogendesigns und der Interviewstruktur:} Die derzeitige Struktur des Experiments erlaubt es Kandidaten, durch Wiederholungen von Informationen oder Technologien, die der Bot vorgibt, zu hohe Bewertungen zu erzielen. Es besteht daher die Notwendigkeit, sowohl die Fragen als auch die Bewertungslogik weiter zu verfeinern.
    \item \textbf{Vermeidung von Bias in der \acs{KI}-gestützten Bewertung:} Es wurde beobachtet, dass der Bot in einigen Fällen zu unkritisch ist und keine ausreichende Differenzierung zwischen den Kandidaten vornimmt. Dies könnte zu Verzerrungen führen, die besonders bei der Bewertung von Soft Skills und technischen Fähigkeiten auftreten.
    \item \textbf{Bewertung von Soft Skills durch die \acs{KI}:} Die Analyse der Ergebnisse zeigt, dass das \acs{LLM} Schwierigkeiten hat, Soft Skills wie Teamfähigkeit oder Konfliktlösung realistisch zu bewerten. Diese Fähigkeiten sind oft nuanciert und kontextabhängig, was dazu führt, dass die \acs{KI} zu positive Bewertungen vergibt.
    \item \textbf{Notwendigkeit menschlicher Überwachung:} Es hat sich gezeigt, dass eine rein \acs{KI}-basierte Bewertung nicht ausreicht, um eine präzise Vorauswahl zu gewährleisten. Eine manuelle Überprüfung oder Nachjustierung der Bewertungen durch menschliche Experten bleibt notwendig, um sicherzustellen, dass die Entscheidungen fundiert sind.
\end{itemize}

\subsection{Vorschläge zur Verbesserung}

Auf Basis der genannten Herausforderungen wurden mehrere Vorschläge zur Verbesserung der Evaluationsmethodik entwickelt:

\begin{enumerate}
    \item \textbf{Einführung einer fiktiven Aufgabe mit Musterlösung:} Um die Fähigkeiten der Kandidaten präziser bewerten zu können, könnte eine fiktive Aufgabe eingeführt werden, die während des Interviews bearbeitet wird. Diese Aufgabe sollte eine Musterlösung enthalten, die als Vergleichsmaßstab für die Kandidatenantworten dient. Dies würde es dem \acs{LLM} ermöglichen, die tatsächliche Problemlösungsfähigkeit und das technische Verständnis der Bewerber besser einzuschätzen.
    \item \textbf{Anpassung der Bewertungsskala:} Eine Neuberechnung der Bewertungsskala wurde vorgeschlagen, um eine klarere Unterscheidung zwischen geeigneten und nicht geeigneten Kandidaten zu ermöglichen. Bewerber, die unter einer Punktzahl von 5 liegen, sollten als ungeeignet eingestuft werden, während Punktzahlen zwischen 6 und 7 als \enquote{okay} gelten und Bewerber über 7 als \enquote{gute Kandidaten} eingestuft werden. Diese Skalierung würde zu einer präziseren und gerechteren Bewertung führen.
\end{enumerate}

Zusammenfassend lässt sich sagen, dass das Experiment vielversprechende Ansätze aufzeigt, aber weitere Optimierungen in der Struktur und den Bewertungsmechanismen erforderlich sind, um die Qualität der Bewerbervorauswahl durch das \acs{LLM} zu erhöhen. Ein stärker gewichteter Fokus auf fachliche Kompetenz und die Einführung spezifischer Aufgaben könnten dazu beitragen, die Ergebnisse weiter zu differenzieren und präzisere Rückschlüsse auf die Eignung der Kandidaten zu ziehen.